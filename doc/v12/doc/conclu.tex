\chapter{Conclusions}
\label{cha:conclu}

\section{Recommendations}

\subsection{Prefer Sparse Matrix Algorithms}

This is perhaps the most obvious recommendation that comes out of this
work: Even if the data fits in memory (as it happens here), it is a waste
of CPU resources to compute against the zero values of the \gls{Laplacian}.

\subsection{If dense, go with \gls{MRRR}}

If for some reason (like a prohibition to perform drastic code
changes), the application needs to go with dense matrices, at least
they could use \gls{MRRR} (the best algorithm available). This can
be obtained from opensource implementations of LAPACK \footnote{There
  are also commercial, hardware optimized versions like Intel's, which
  could be considered.}, though Java
Native Interface (JNI) will be needed to call the routines from Java (which
is the applications' language). Alternatively, they can try the
Java-Netlib opensource project \cite{jnetlib}, which offers Java
wrappers against BLAS, LAPACK and other native libraries.

\subsection{If development time is a constraint, consider Lanczos/\gls{IRLM}}

If they can invest more time (probably some weeks), and assuming
they are allowed to introduce more drastic changes like the usage of
sparse matrices, then Lanczos/\gls{IRLM} is the way to go. Its ARPACK
implementation is also opensource, and also readily available through
the Java Netlib project. Of course the recommendation would be to use
it in combination
the \gls{Cholmod} linear solver, though that may add more time (in case JNI
wrappers do not exist yet).

\subsection{If speed is a concern, go with \gls{LOBPCG}}

Finally, if they are willing to invest even more time (some months
perhaps),  and if speed is the
main concern, then they can explore the winner of the competition:
\gls{LOBPCG}. Porting the Python code from Scipy into Java \footnote{An
  interesting note about this code, is that the algorithm creator
  himself (Knyazev), helped with its development. This speaks about
  its quality.} may certainly be a non trivial task, but the impact
could be minimized if they restrict their attention to the single
vector version. Ultimately, that version is all we need to compute the
\gls{FiedlerVector} (the   
actual code implements a block version that can approximate multiple
vectors, but we do not need that). \\

Another option to explore, shared by Knyazev \footnote{Who kindly
  replied quickly to this, and other inquiries we had about his
  algorithm.}, is to consider the existing Java ports of
\gls{LOBPCG}. In particular, there is one implementation
available at
\url{https://github.com/bahaelaila7/sparse-eigensolvers-java}, which
comes from Rico Argentati. He is one of Knyazev's former students and also
co-author of BLOPEX \cite{blopex} (the original native
implementation of the \gls{LOBPCG}).

\subsection{Use an specialized algorithm for computing SCC}

As explained on the \cref{sub:avoid-clust-eigv}, the phenomenon of
\gls{ClusteredEigenvalues} translates into a disconnected graph, and
it seems better for the algorithms to prevent such cases as input. The
application already detects these cases, but it does so by using the
byproducts of the eigenproblem it already solves. But if we want to
leverage the more advanced sparse matrix algorithms, is better to use
a more specialized method for detecting the disconnected graph, to
be called before the eigensolver. In
concrete, we propose the one documented in \cite{pearce05} (whose
implementation performed good enough in our tests, offering sub-second
times for the biggest matrices). 

\section{Additional considerations}

\subsection{Need for sparse format}

As explained in previous chapters, we 
could have used either CSR or CSC formats, but the need to compute the
\gls{SCC} of the underlying graph, made us pick the CSR
format (see \cref{sub:avoid-clust-eigv}). If they are going with
either \gls{IRLM} or \gls{LOBPCG}, this is the format we recommend to use. \\


The matrix format selection will not come for free, as it brings
some development costs. 
This is because the application currently uses a serial version of the
Colt library \cite{colt}, which does not 
seem to offer any sparse format. Thus, additional effort will be
needed to research which libraries support CSR in Java. And from those
found, we will need to seek which one allows a better integration with
the Colt library. This integration could be minimal and just become a
conversion of format (from the dense one used by Colt, to the sparse
CSR).  \\

Let us recall that the sparse matrix algorithms will not care if the caller uses
CSR or not, as they do not require an explicit representation of the
matrix. However, while implementing the callback mechanisms that 
compute using the \gls{Laplacian} (either $L\vec{x}$ or solving $L\vec{y} =
\vec{x}$), we should definitely use the sparse format, otherwise the
promised gains will not show up.

\subsection{Port of the algorithm for computing the \gls{SCC}}

The algorithm used for efficiently computing the \gls{SCC}, along with the
code to recalculate the weights matrix (see
\cref{sub:avoid-clust-eigv}), is all in Python (probably 
using other underlying native libraries). Assuming we manage to make the CSR
format available to Java, another task to include will be to look for
a port of this algorithm (or to implement it using the article
\cite{pearce05}). While this task may look non trivial, at least does
not require deep knowledge of Numerical Analysis, which should make it
tractable by regular programmers \footnote{We did not use the
word ``regular'' in a pejorative sense, of course. We just try to make
the point that ``regularly'', programmers working in corporations do
not know about Numerical Analysis but rather about more standard
Computer Science topics.}.

\section{Challenges found during this thesis}

Finally, we would like to dedicate a few lines about some challenges
that were found during the elaboration of this 6 months thesis
project (part time).

\subsection{Numerical Linear Algebra is hard}

For the non initiated, meaning students who have not taken advanced
courses in Linear Algebra,  Numerical Linear Algebra or Numerical
Analysis in general, is quite a challenge to grasp the algorithms
explanations found in literature. There seems to be a tradition to
assume that the reader has this immense background, as many details
are omitted. Therefore, one needs to go back to more elementary
material to try filling some of the gaps. This makes the overall
progress a bit slow, and that is the reason why only three algorithms
were presented (more on this below). \\

We dared to embrace this topic, thinking naively that a previous
course project (related to the SVD factorization), would provide
the required context, but this was 
true just partially. The specialization required to fully grasp the
theory behind the three algorithms compared in this work, goes far beyond
the context acquired during the last year (which includes the last
Master's courses and the elaboration of this thesis). That is why we
reduced the scope to understand only the main ideas, and to learn how
to use in practice the implementations.


\subsection{There is a zoo of algorithms out there}

The three algorithms compared in this thesis, definitely do not represent
the entire set of options available. Even if we restrict ourselves to
the properties of the \gls{Laplacian}, and to serial execution, there are
several more options that offer opensource implementations to
explore. Just to give an idea of the diversity available online, the
following is a partial list of additional options we also considered
(meaning that we read a bit about the algorithms, that we installed
the software and tried it at least once):

\begin{itemize}
  \item TRACEMIN-fiedler: A parallel algorithm for computing the
    \gls{FiedlerVector} \cite{trminfiedler} \footnote{There is no
      opensource package for this algorithm, but we contacted directly
    the author to get a copy of the code used in the article.}.
  \item Anasazi: Software for the numerical solution of large-scale
    eigenvalue problems \cite{anasazi}.
  \item Slepc: Scalable and flexible toolkit for the solution of
    eigenvalue problems \cite{slepc}.
  \item Primme: preconditioned iterative multimethod
    eigensolver-methods and software description \cite{primme}.
  \item Lanczos/\gls{IRLM} from ARPACK, but using iterative linear solvers
    (like those reported in \cite{martinez16} \footnote{In practice we
  did not find an implementation for the preconditioners mentioned in
  the article \cite{martinez16}, but tried with PyAMG \cite{pyamg}}). 
\end{itemize}

All the options from the above list are quite interesting to explore, 
along with the publications about their theoretical
foundations. Except for the last one (for which we did not find an
available implementation), the rest were tried indeed. But we did
not have enough time to research more about their inner workings, nor
to understand how to use them in practice to compute the
\gls{FiedlerVector}. If more time becomes available, it would be an interesting
follow up for this thesis, to explore the options listed above.

