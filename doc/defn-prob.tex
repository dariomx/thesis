\chapter{Definition of the Problem}
\label{cha:defn-prob}

One of the main drivers for picking the thesis topic, was to enhance a
real life software product. The quest eventually
brought to the table an application which needs to calculate
the eigen decomposition for certain matrices; and which was finding such step
as a potential bottleneck. To give a concrete example, a matrix of  $867
\times 867$ was taking 14 seconds on commodity hardware; and the
application needed to perform a serious of them in a row, which
potentially could accumulate to one minute or more. Those calculations are
to be done in the context of a single user request, for implementing
a higher level functionality which usually takes a few milliseconds.
The concern raises further, with the future expectation of handling
much bigger matrices. The eigen decomposition is performed using Colt
library (\cite{colt}).\\ 

We adopted above problem as the thesis topic, and the purpose of this
chapter is to document the refinement of the
requirements we got, as well additional details of
the application (not too many though, as we are talking about proprietary
software). 

\section{Initial requirements}

The raw description of the problem was to speed up, as much as
possible, the eigen decomposition mentioned above. But
there were several restrictions or special requirements around such
calculation, which we list below:

\begin{itemize}
  \item The computation needs to be performed in the Java programming
    language, as the whole application is written in the same. 
  \item The executing hardware is a commodity computer using Intel
    processors (exact definition of the machine used for testing
    appears on \cref{cha:exper}).
  \item We can not use multi-threading, the computation needs to occur
    serially (this relates to the application architecture, but we can
    not disclose more details about it). 
  \item The matrices are symmetric (they are equal to its transpose). 
  \item All the eigenvalues are required, but only one eigenvector.
\end{itemize}
\hfill

The above list of requirements suggested that we needed to restrict
our attention to serial algorithms for the Symmetric Eigen
Decomposition (proper mathematical definitions will be provided on
\cref{cha:symm-eigen}). Such reduced scope was welcomed, as the whole
topic of Eigen Decomposition (usually called Eigenvalue Problem in
literature), is extensive enough to make its exploration prohibitive.
Another issue is that a big part of the literature focuses on
parallel/distributed algorithms, but their serial versions could be
still attractive for our purposes and hence we can not be entirely
discarded. \\  

But the Symmetric Eigenvalue Problem is still too wide. To give
the reader a sense of its immensity, is enough to mention that a
restrictive search by title in Google Scholar
\cite{googlescholar}, brings almost 1000 references which contain the
words ``symmetric'' and ``eigenvalue''. Some of those references
are quite recent, which suggest it is an active area of research. That is
not a surprise given the recent trends of Big Data, Machine Learning
and Data Mining; which heavily rely on efficient Numeric Linear
Algebra algorithms. \\

The following sections detail a refinement of the initial
requirements, aiming to reduce the scope of our algorithm search. 

\section{The context of the calculations}

\subsection{Spectral Clustering}
Digging further into the application code, to which we got access
granted, allowed us to realize that the requirements were actually
more specific. The reason why all eigenvalues were required, was in
order to detect how many of them where zero; to be more concrete,
whether we had more than one being zero \footnote{In real-life
  computer calculations, we do not really compare exactly against
  zero but with a quite small number; that represents a reasonable
  approximation to zero.}. These details
will not make much sense, unless we understand a bit more of the
application details. \\

The software application we are trying to optimize implements, among
many other things, some clustering
algorithms. The term Clustering might
ring a bell to the reader, as it is a trendy topic nowadays; specially
with algorithms like K-Means (see for example \cite{rajaraman14}, a
quite pragmatic 
introduction to Data Mining at large scale, which devotes one chapter
to Clustering). The technique used by our application is a bit
different though, to the classical approaches like K-Means Algorithm;
it uses something called Spectral Clustering, where information about the
eigenvalues and eigenvectors of an special matrix called the Laplacian
\footnote{Built out of the domain data.}, are used to find the clusters
within the data. A gentle introduction to Spectral Clustering can be
found in \cite{luxburg07}, and we will define the Laplacian
in the next section. \\

\subsection{Algebraic Graph Theory}

Let us now talk a bit about an apparently disconnected topic: enter
Algebraic Graph Theory, which is a fascinating field that 
applies tools from Algebra to the understanding of graphs
(the canonical text books are \cite{biggs93} and \cite{godsil01}). The field
is rich enough to have subdivisions, and the one concerning us here is
the application of Linear Algebra machinery against matrices
associated to graphs; in particular the usage of eigenvalues or
eigenvectors of those matrices. Such branch is also called
Spectral Graph Theory, for which \cite{brouwer12} offers a compressed but
comprehensive overview. \\

Let $G = (V,E,W)$ be a weighted undirected graph, where $V$ is the set
of vertices, $E$ is the set of edges and $W$ is an $n \times n$ matrix
with the positive weights associated to the edges ($n = |V|$). The
matrix used in Spectral Graph Theory, that we care about, is
called the Laplacian ($L$); its definition follows below: 

\begin{align*}
  L_{ij} &= \twopartdef{d_i}{i = j}{-w_{ij}}{i \ne j} \\\\
    \suchthat & d_i = \sum_{j=1}^n w_{ij}
\end{align*}
\hfill

A more compact definition of the Laplacian is $L = D - W$, where $D =
diag({d_1,d_2,\dots,d_n})$. There are actually two flavors of the
Laplacian, and the one defined here is referred as \emph{Unnormalized}
Laplacian in literature. \\

The Laplacian is a quite interesting matrix, in the sense that its
eigenvalues (also known as ``spectra''), reveal useful information about the
underlying graph G. In particular, about its connectivity. In this
regard, the theorem below will be cited later on the context of our
application: \\

\begin{theorem}
  \label{thm:speconn}
  The algebraic multiplicity of zero as an eigenvalue of $L$ equals
  the number of connected components of $G$.
\end{theorem}
\hfill

Let us recall that the algebraic multiplicity \footnote{For
  symmetric matrices like the Laplacian, the algebraic multiplicity
  equals the geometric multiplicity; where the later is defined as the
  dimension of the subspace generated by the associated
  eigenvectors. In that sense, for symmetric matrices there is 
  no ambiguity and literature simply talks about the \emph{multiplicity} of
  the eigenvalues.} of an eigenvalue, is the
number of times that it appears as a root of the characteristic
polynomial; while a connected component is a subgraph of
$G$ such that all its vertices are connected, but that as whole is not
connected with the rest of the graph. 
A condensed proof of this theorem is provided in
\cite{brouwer12}, and a more detailed one appears in \cite{luxburg07}
(though not precisely in the context of Algebraic Graph Theory). \\

\subsection{The Connection between the two fields}

What does Spectral Clustering (performed by our application), have to
do with Algebraic Graph Theory? The former borrows its tools from the
second. As \cite{jia14} explains, the idea of Spectral Clustering is
to reduce the problem of data clustering to one of graph
partitioning. It does so by constructing an undirected graph with each
point in the dataset being represented as a vertex, and by defining a
similarity function 
between the points (vectors in \R{n} that represent our data items). Such
function serves to build the weight matrix $W$, of the undirected weighted graph
$G = (V,E,W)$ we mentioned earlier \footnote{A typical choice
  for the similarity function is the Euclidean Distance, but there are
  other choices; see \cite{luxburg07} for further
  details.}. Thus, we can perform partitions on the graph
based on certain cut methods, and we perceive the resulting components as
clusters (whose vertices reference original points in \R{n}, that
compose our dataset). \\

It is on those cut methods, that the theorems of Algebraic
Graph Theory are used. The particular application we care about,
performs a bi-partition by employing the so called Fiedler Method; in
honor to Miroslav Fiedler, who wrote a seminal paper on both Algebraic
Graph Theory and Spectral Clustering, describing the actual
method. The idea is to calculate the eigenvector associated to the
second smallest eigenvalue of the Laplacian matrix $L$ (called Fiedler
Vector), and use the sign of 
its elements to split the vertices of the graph in two
partitions. \\

The technical details are in Fiedler's article
(\cite{fiedler73}), or in standard texts of Algebraic Graph Theory.
The intuitive idea is that the partitions generated by the Fiedler
Vector are ``good'', because they minimize the number of edges
required to split the graph; and to some extent the amount of edges
joining the two partitions, tells us how close or distant are the
clusters that they represent in the original dataset. Actually,
Fiedler gave us an explicit metric for this connectivity of the graph;
it turns out that the eigenvalue associated with the Fiedler Vector
(second smallest eigenvalue), tells us how connected or disconnected
the graph is. Because of this, Fiedler baptized this eigenvalue as the
Algebraic Connectivity; the closer to zero it gets, the more
disconnected the graph is. \\

The following theorem refers to the newly introduced concept of
Algebraic Connectivity, and serves as a further link between the two
branches of Mathematics that we briefly introduced: \\

\begin{theorem}
  \label{thm:algconn}
  The Algebraic Connectivity of $L$ is zero $\iff$ $G$ is disconnected. 
\end{theorem}

This theorem is actually a corollary of \cref{thm:speconn}, for
the particular case of a connected graph. This is because the
Laplacian $L$ (a real, symmetric and positive semi-definite
matrix \footnote{A symmetric $n \times n$ positive semi-definite matrix $M$, is
  related to a 
  quadratic form and has the property of $\trans{\vec{x}}M\vec{x} > 0
\,, \forall x \in \R{n}$. See \cite{strang88} for a review of this concept.}), exhibits additional properties: \\ 

\begin{itemize}
  \item All its eigenvalues are real and $ \ge 0$ (see \cite{strang88}). \\
  \item Zero is always its smallest eigenvalue (see \cite{luxburg07}).
\end{itemize}
\hfill

If we apply \cref{thm:speconn} to the case of a graph with a single
component (connected graph), then it would tell us that zero appears
once as an eigenvalue of its Laplacian. Using the properties listed above, we
know that the second smallest eigenvalue (Algebraic Connectivity)
needs to be either zero or a 
positive number; but as we just stated that it can not be zero again, the
only other option is to be positive. That could be translated into
proposition  ``A graph is connected $\iff$ it has an algebraic
connectivity $> 0$''; which can be reworded as follows if we negate both sides of
the logical equivalence: ``The
Algebraic Connectivity of a graph is zero $\iff$ it is
disconnected''. Latest rewording is essentially \cref{thm:algconn}.

\section{Final requirements}

The previous sections and the two theorems mentioned, were more than
just a cosmetic exercise; they have the practical purpose of
understanding better the current requirement, and also of proposing a
refinement that will help us to reduce the scope of our bibliographic
search. \\

On the context of Spectral Clustering, there is certain code in the
application that does a bi-partition of a graph. It does so by
calculating the Fiedler Vector, and splitting the nodes depending on
the sign of the vector elements. However, prior doing the
bi-partition, the application needs to verify if the graph is
connected (the Fiedler Vector will not help much if the graph has more
than one connected component). In order to detect the disconnected
case, the code leverages \cref{thm:speconn} and asks how many
eigenvalues of the Laplacian are (nearly) 
zero. If there are more than one, then we have a disconnected graph
(which requires a different treatment); if there is just one, we
proceed to split the graph using Fiedler Vector. The pseudo-code of
this critical section would look like this: \\

\begin{algorithm}
  \label{alg:orig-code}
  \caption{Original calculation of the graph partition}
%
  \setstretch{1.35}
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
  \DontPrintSemicolon
%
    \Input{Laplacian matrix $L$ of the graph}
%
    \Output{A partitioned graph}
%
    \text{calculate all eigenvalues and eigenvectors of $L$} \;
%    
    \If {\text{more than one nearly zero eigenvalue}}
    {
      \text{special treatment for disconnected $G$ using all eigenvalues/eigenvectors} 
    }
    \Else
    {
      \text{do regular bi-partition of connected $G$ using Fiedler
        Vector} 
    }
\end{algorithm}
\hfill

Under the assumption that the connected case is more common than the
disconnected one, we can propose the first high level
optimization. Theorem \cref{thm:algconn} tells us that, in order to
detect the disconnected case, we do not need  to calculate all the
eigenvalues of $L$; the Algebraic Connectivity (second smallest) is
enough. Thus, the proposed new logic can be seen in
\cref{alg:optim-code}. For the cases of disconnected graphs, we would need to
calculate twice the Algebraic Connectivity and Fiedler Vector; but
those are expected to be rare occasions. 

\begin{algorithm}
  \label{alg:optim-code}
  \caption{Proposed calculation of the graph partition}
%
  \setstretch{1.35}
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
  \DontPrintSemicolon
%
    \Input{Laplacian matrix $L$ of the graph}
%
    \Output{A partitioned graph}
%
    \text{calculate algebraic connectivity and fiedler vector of $L$} \;
%    
    \If {\text{algebraic connectivity is nearly zero}}
    {
      \text{calculate all eigenvalues and eigenvectors of $L$} \;      
      \text{special treatment for disconnected $G$ using all eigenvalues/eigenvectors} 
    }
    \Else
    {
      \text{do regular bi-partition of connected $G$ using Fiedler
        Vector} 
    }
\end{algorithm}
\hfill

With the proposed change we are opening the door for leveraging the
properties of the Laplacian matrix $L$, which is symmetric, sparse and
positive semi-definite. While we have mentioned what it means to be
symmetric ($L = \trans{L}$) and positive semi-definite ($\forall x \in
\R{n} | \trans{\vec{x}}L\vec{x} > 0$); the sparsity deserves a few
words. Intuitively, an sparse matrix has ``mostly zeros'', though that
is a quite open definition subject to arbitrary interpretations; how many
are ``mostly''? According to \cite{richard12}, in practice a matrix
with more than $50$\% of zero entries can be considered sparse. The
sample $867 \times 867$ matrix we mentioned on the first section,
which is expected to be a representative input, falls into this
definition; it has around $70$\% of (nearly) zero entries. This
sparse characteristic of the Laplacian is quite relevant for our
investigation, as the numeric algorithms for solving the Symmetric
Eigenvalue Problem are clearly divided in two big branches: those
attacking the non sparse (dense) matrices, and those covering the
sparse ones. \\

Furthermore, the algorithms specialized for sparse matrices tend to
focus on calculating just a few eigenvalues/eigenvectors; that fits
perfectly well on the proposed logic of \cref{alg:optim-code}, which
usually requires a single 
pair. This does not necessarily mean that we are discarding
the dense-matrix branch of algorithms. For the rare but still possible
case of a disconnected graph, we want to use something better than
current implementation in the Colt library; as it uses the classical
algorithm known as \emph{QL} (see \cref{sec:trid-ql}); based on routine
\emph{TQL2} of the
 Fortran77 library EISPACK (\cite{eispack}). Such library has been
superseded by LAPACK (\cite{lapack}); and in particular, the
equivalent of routine \emph{TQL2} has been improved (see
\cref{sec:mr3}). As the sparse routines are designed for calculating a
few eigenvalues/eigenvectors, it would make sense to continue using a
dense matrix algorithm for the disconnected case, but leveraging
modern LAPACK implementations (see \cref{cha:lapack}). \\

Going further, there are algorithms specifically tailored for
calculating the Fiedler Vector and its associated eigenvalue
(Algebraic Connectivity). We also want to include those in the
comparison (see \cref{cha:trmin-fiedler}). \\

A final consideration is that, while the serial nature of the
algorithm remains as a requirement (no parallelism), there is no
restriction for recommending the usage of vectorized kernels like BLAS
(\cite{blas}); which is actually used by LAPACK and other higher level
libraries. BLAS (Basic Linear Algebra Subroutines), is a mature
industry standard, that hardware vendors implement for their
processors; in particular Intel leverages its vector facilities SSE
(Intel seems to be the commodity processor that the application can
rely on). \\

Taking into account the new understanding of the problem, we proceed
to list the refined requirements: 

\begin{itemize}
  \item Goal is to reduce the time spent solving the Symmetric Eigenvalue
    Problem. 
  \item The matrix against which we compute the eigenvalues and
    eigenvectors is the Laplacian of a graph; expectation is to seek
    for algorithms that leverage its qualities (symmetric, sparse and
    positive semi-definite). 
  \item The computation needs to be performed in the Java programming
    language, as the whole application is written in the same. 
  \item The executing hardware is a commodity computer using Intel
    processors (exact definition of the machine used for testing
    appears on \cref{cha:exper}).
  \item We can not use multi-threading, the computation needs to occur
    serially. There is no restriction
    though, in suggesting the usage of vectorized routines for Intel
    processors; like those present in BLAS library. 
  \item Even if the disconnected graph case is uncommon, is
    still possible and demands the calculation of all eigenvalues and
    eigenvectors. We want to use though, a newer algorithm for dense
    matrices than the one implemented in Colt (like those present in
    LAPACK library). 
  \item For the common case of a connected graph, we shall calculate
    only the algebraic connectivity and its associated Fiedler
    Vector (see pseudo-code from  \cref{alg:optim-code}).
\end{itemize}
\hfill

\section{Expectation and outline of the thesis}
Being this a master degree thesis, is not expected nor feasible due
time constraints, that we come up with a new algorithm (that would be
suitable for a PhD thesis). Then, expectation was to search what are
the available algorithms (feasible to test in Java); and to evaluate
their performance under the cited requirements. Detailed descriptions
of the algorithms are to be provided, along with proper explanations about
why some performed better than others. Ultimate expectation is that,
with the suggested optimization, the application becomes able to cover
new use-cases involving bigger data sets (bigger Laplacian
matrices). \\

The Java restriction had both a dark and a bright side; the dark one
is that we could not try the most advanced solvers like PRIMME 
(\cite{primme}), Slepc (\cite{slepc}) or Anasazi (\cite{anasazi}).
Enabling those for Java usage through JNI might be possible (assuming
we avoid their parallel/distributed versions), but the time
constraints for this project prevented us exploring that path. The
bright side is that, having as requirement the immediate testability
in Java, turned out to be the best filter to reduce the immense volume
of algorithms available in literature. \\

The outline of the rest of this thesis is as follows: 

\begin{itemize}
  \item \Cref{cha:symm-eigen} gives an introduction to the generic problem
    we want to solve faster, The Symmetric Eigenvalue Problem. We also
    include here, a brief overview of the numerical aspects that need
    to be considered when evaluating the algorithms (numerical
    stability and ill-conditioned data). 

  \item \Cref{cha:lapack} covers three dense-matrix algorithms that come bundled with
    LAPACK, the first one is the QR algorithm; which is not really one
    of the proposed candidates, but rather the theoretical background
    of the Colt implementation. This chapter also includes the first two
    candidates for the experiment phase: the MRRR and the
    Divide-and-Conquer algorithms; both included as well in LAPACK. 

  \item \Cref{cha:ir-lanczos} covers the first sparse-matrix algorithm to
    evaluate, the Implicitly Restarted Lanczos Algorithm (available in
    a Java thanks to a port of ARPACK \cite{arpack}). 

  \item \Cref{cha:lobpcg} introduces the second sparse-matrix proposal,
    the Locally Optimal Block Preconditioned Conjugate Gradient
    Algorithm (LOBPCG). This is perhaps the most modern algorithm of
    our candidates, and
    the only one incorporating preconditioning of the data. The Java
    implementation is available thanks to Sparse Eigensolvers for Java
    Project (\cite{sparseigensolvjava}). Special care this
    implementation deserves, as the project seems abandoned. 

  \item \Cref{cha:trmin-fiedler} explains the most specialized candidate
    of the proposal, the TraceMin-Fielder Algorithm; which was designed
    specifically for calculating only the algebraic connectivity and
    the Fiedler Vector. The expectation is that this algorithm beats the rest
    during the experiments, as is the one taking into account more
    properties of the matrix $L$ (see \Cref{cha:exper} to find out if
    this expectation matched reality). There is no Java implementation
    available, but we will port the Python version found in Networkx
    Project \cite{networkx}. This is the only algorithm where we
    considered doing the port ourselves, both due the high level nature
    of existing implementation (Python) and because is the most
    promising candidate.

  \item \Cref{cha:exper} describes the experimentation phase, including
    the details of the commodity hardware, as well as the matrices
    used for comparing the different algorithm
    implementations (we included the provided $867 \times 867$
    matrix, but also other ones from standard benchmarks
    available). The performance observed (memory and cpu) are reported
    on this chapter, across the different combinations of algorithms
    and matrices. Besides performance, we also evaluate the quality of
    the solutions; comparing against the results given by Colt library
    (which may not be the most accurate one, but at least represents
    the current results used by the application). 

  \item \Cref{cha:conclu} has the conclusions which interprets the
    results, explaining why certain algorithms performed better than
    others, under the experiment setting. It includes the final
    recommendation for the application stakeholders.
\end{itemize}
