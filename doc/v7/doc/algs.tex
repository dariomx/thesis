\chapter{The Algorithms}

In this chapter we deal with explaining the algorithms that we
compared, aiming to provide our recommentation for the
application. The theory behind these algorithms is dense and hard to
grasp for the non-initiated in Numerical Linear Algebra; hence, given
the time constraints we had for making this thesis project, we focused
on the following points only:

\begin{itemize}
\item Main ideas of the algorithms, seeking to compare them.
\item How to use their implementations in practice.
\item Restrict the search to serial versions of the algorithms
  (application requirement).
\item Look for algorithms that leverage the Laplacian's properties
  (Symmetric, Sparse, Positive-Semi-Definite and with first eigenpair
  $(0,\vec{1})$). The more the algorithms took into account these, the
  better they performed in practice. 
\end{itemize}

\section{Preliminaries}

This section introduces the common mathematical tools used by the
algorithms.

\subsection{Power Method (for Symmetric Matrices)}

The most primitive algorithm for computing eigenvectors, is the so
called Power Method; which compensates its limitations with its
extremely easy to understand definition. It can be summarized by the
two steps below (where $\vec{x}$ is the initial approximation to the
wanted eigenvector):

\begin{equation*}
    \vec{x}_0 = \vec{x} \ds{\ds{\land}}
    \vec{x}_{k+1} = A\vec{x}_k  
\end{equation*}
\joinbelow{1cm}

The above iteration, surprisingly, converges to the eigenvector
associated to the biggest eigenvalue of the matrix $A$ (or to the one
associated with the smallest eigenvalue, if one uses $\inv{A}$ instead
\footnote{The presence of the inverse matrix $\inv{A}$ is mostly due
  notation, in practice we do not compute $\inv{A}x$; instead we solve the
  associated linear system $A\vec{y} = \vec{x}$.}). There is a
generic proof of this on \cite{saad92}, but a more accesible argument
is presented below for the particular of symmetric matrices. The vectors
$\vec{v}_i$ are the eigenvectors of the matrix:

\begin{align*}
  \vec{x}_k &= A^{k}\vec{x_0} \\
  &= A^{k}\left(\sum\limits_{\tiny{i=1}}^{n} \alpha_i\vec{v}_i \right) \\
  &= \sum\limits_{\tiny{i=1}}^{n} \alpha_i A^{k} \vec{v}_i  \\
  &= \sum\limits_{\tiny{i=1}}^{n} \alpha_i \lambda_i^{k} \vec{v}_i \\
  &= \alpha_n \lambda_n^{k} \vec{v}_n +
  \sum\limits_{\tiny{i=1}}^{n-1} \alpha_i \lambda_i^{k} \vec{v}_i \\
  & \overbrace{\approx}^{k \to \infty} \beta \vec{v}_n  
\end{align*}
\joinbelow{cm}

The key argument for the development above is that we assumed the
matrix was symmetric, which implies its eigenvectors form a full
basis; this means any vector, including $x_0$, can be expressed as a
linear combination of them (see \cite{strang88} or \cite{golub13} for
more details). \\

The Power Method does not always converge, but here we focus on its
more commonly found limitation: slow convergence. This is because it
relies on the eigengap (distance between the desired eigenvalue and
the next one on that side of the spectra\footnote{Spectra is a
  commonly found name for the set of eigenvalues of a matrix.}). To be
more concrete, the convergence is Power Method is proportional to
$\dfrac{\lambda_{n-1}}{\lambda_n}$ or $\dfrac{\lambda_1}{\lambda_2}$;
depending on whether we are seeking the smallest or biggest
eigenvector \footnote{This is an abuse of the language of course, when
we say the smallest or biggest eigenvector or eigenpair, we actually refer to
the property of the associated eigenvalue.}. This limitation will appear
later, when comparing the algorithms.

\subsection{Krylov Subspaces}

The next tool is Krylov Subspaces, which are used to search for
approximations of the desired eigenvectors. A Krylov subspace of
dimension $m$, for a given matrix $A$ and generating vector
$\vec{x_0}$, is defined as follows: 

\begin{equation*}
    \Krylov{A}{\vec{x}_0}{m} =
    \func{span}
    \left\{
      A^0\vec{x}_0\ds{,} A^1\vec{x}_0\ds{,} \ldots \ds{,}  A^{(m-1)}\vec{x}_0
      \right\}  
\end{equation*}
\joinbelow{1cm}

Both \cite{parlett80} and \cite{saad92} have chapters with more
properties about Krylov Subspaces; but the intuitive idea is that the
Power Method kind of wastes a lot of information: the iteration
produces the vectors $A^k\vec{x_0}$, but only the last one is actually
used. Krylov Subspaces keeps all these vectors, and use them to
generate a subspace where there are more changes to find good
approximations for the eigenvectors.

\subsection{Rayleigh-Ritz Method}

This is another way for extracting eigenvectors. Assuming that you
already have a subspace where you want to search for the
approximations, then the high level algorithm looks like this:

\begin{itemize}
  \item Compute orthonormal basis $V$ of subspace. 
  \item Solve (smaller) eigenproblem $R\vec{y} = \lambda\vec{y}
    \ds{\suchthat} R = \trans{V} A V$. 
  \item Compute the Ritz pairs
      $(\apx{\lambda}_i,\apx{\vec{x}}_i) = (\lambda_i, V\vec{y}_i)$
\end{itemize}

As you can see on the above steps, in order for this method to work,
the dimension of this subspace  needs to be much smaller than
$\func{dim}(A)$; because you are ultimately solving an small
eigenproblem anyway.  The rationale is that it becomes easier to
solve the eigenproblem on a much smaller dimension \footnote{As we
  will see later, algorithms targetting large sparse matrices can
  ultimately rely on dense matrix algorithms; with the premise that
  they do so at a much smaller scale.}. \\

Another interesting detail to note, is that the
matrix $R$ is something like a projection of the original matrix $A$
on the approximating subspace; the two matrices are pretty much the
same thing (similar) expect for a change of 
basis. This implies the spectra of $R$ is a subset of the spectra of
$A$, so the eigenvalues obtained with Rayleigh-Ritz method can be used
directly as the desired approximations. The eigenvectors need a change
of basis though ($V\vec{y_i}$), which takes them from the coordinates
used on the approximating subspace to the canonical coordinates where
the matrix $A$ operates. More details about the theory behind this
method, can be found in \cite{saad92}.

\section{Dense Matrix Algorithms}

Although the Laplacian is an sparse matrix (70\% of zeros for our
target application), we still consider the family of algorithms that
care about dense matrix representations. This is mostly due comparison
purposes, as the current implementation the application uses today is
based on a method from this family. This will allow us to know how
much we improved the execution time, with the new sparse matrix
algorithms; but we just provide references for 
algorithms in this section, as their details fall out of the scope of
this thesis (our focus is on sparse matrix methods). \\

\subsection{Symmetric Tridiagonal QL Algorithm}
The application currently uses the Symmetric Tridiagonal QL
Algorithm. Like other algorithms from same family, this method
converts first the input matrix into tridiagonal form; as is easier to
compute from there the eigenpairs. Once in this reduced form, the
simplest version of the algorithm could be thought as a block
version of the Power Method; on each iteration we need to
orthogonalize with some matrix factorization (QL) and to multiply the
resulting orthogonal vectors by the matrix in a possibly different
basis. \\

Above is is just the basic idea, practical implementations have far
more points to consider; the reader interested in more details about
this QL Algorithms or its cousins the QR Algorithms, can consult
standard literature on the topic like \cite{golub13} or
\cite{parlett80}. The only additional detail that we want to mention
here, is that this algorithm has an inherent limitation for our
application: it is designed to compute all the eigenpairs. This sounds
like a waste of resources, given that we just one a particular
eigenpair (the second smallest). 

\subsection{The MRRR Algorithm}

The MRRR Algorithm (Multiple Relatively Robust Representations or
$MR^3$), is a more suitable option for computing the second
eigenpair. Is a quite sophisticated procedure, and the reader
can consult the available literature about it like \cite{dhillon97},
\cite{dhillon04}, \cite{dhillon06} or \cite{parlett04}. The only
detail we will mention here, is that this algorithm is capable of
computing eigenpairs in isolation; so it has a considerable advantage
over the previous one. Actually, this algorithm is perhaps the best
the application can do, in terms of dense matrix methods.


\section{Sparse Matrix Algorithms}

If we really want to leverage the properties of the Laplacian, we need
to consider algorithms which are suitable for sparse matrix
representations. They are usually designed to calculate an small
portion of the spectra (smallest or biggest), and the reason why they
work better with sparse representations is because they do not require
an explicit representation of the matrix at any moment. Instead, these
algorithms only require a callback mechanism to perform an operation
involving the matrix (eg $A\vec{x}$); therefore, the internals of
those operations can be optimized to take advantage of the sparse
format in question (instead of using a generic logic that assumes a
dense representation). We will present a couple of algorithms from
this family, a variant of Lanzos and LOBPCG.

\subsection{Lanczos (IRLM)}

Lanczos algorithm is actually a family of them, and the particular
variant we are presenting here is called Implicitly Restarted Lanczos
Method (IRLM). The algorithm is iterative (it progressively approximates the
desired eigenpairs), and the main idea is to apply the Rayleigh-Ritz
method against a Krylov subspace $\Krylov{A}{\vec{x_0}}{m} \suchthat m
> k$. This implies that it needs to solve on each iteration, an small
eigenproblemfor an $m \times m$ matrix $H$; the $m$ is a bit bigger
than the number of desired eigenpairs but still much smaller than the
dimension of the matrix. \\

One of the issues with Krylov Subspaces is that we do not know in
advance, how big the dimension $m$ needs to be in order to contain
interesting approximations to the sought eigenvectors; since such
dimension is in function of the number of vectors $A^k \vec{x_0}$ that
we keep in memory, this can be a serious problem. In order to tackle
this issue, the IRLM uses a tool called Implicitly Restarted QR
Algorithm to apply $p$ shifts against the matrix $H$:

\begin{equation*}
    j=1 \ldots p:\, QR = \func{qr}(H - \lambda_j I) \land H = \trans{Q}HQ
    \ds{\suchthat}
    m = k + p
\end{equation*}
\joinbelow{1cm}

After the shift above a truncation of $k \times k$ takes place, and on
next iteration the matrix $H$ is filled up again to a size $m$ (using
powers of the matrix, among other stuff). The intuitive idea is that
at any iteration we have $m = k + p$ eigenpairs from matrix $H$, where
$k$ is the actual number requested by the user and $p$ is the number
of extra eigenpairs we have. The purpose of the procedure above is to 
discard the p vectors from $H$ that do not contribute to the
interesting eigenvectors. The definite reference for all the gory
details of this variant of the Lanczos algorithm can be found at
\cite{arpack}. \\

As far as practical usage of the IRLM routine, we took the following
considerations:

\begin{itemize}
\item We set $k=2$ and by default the implementation sets $m=2k+1$.
\item Used shift-invert mode with $\sigma=0$ (as we want the smallest
  eigenpairs), and one linear sparse solver; we tested two, SuperLU
  (\cite{superlu97}, \cite{superlu05}) and  Cholmod (\cite{cholmod08},
  \cite{cholmod08a}). These solvers use respectively the LU and
  Cholesky factorizations, hence are considered direct methods; while
  both are specialized for sparse matrices only Cholmod leverages
  Laplacian properties (we will see in the results chapter, how much
  that affects the performance). The linear solver we pass when invoking the
  routine is quite important, as it accounts for $\approx \%80$ of
  the execution time.
\item We observed slow convergence for clustered eigenvalues, but this
  was expected (is a known limitation inherited from the Power
  Method).
\end{itemize}

Last thing to mention about this variant of Lanczos, is that it
usually appears in Spectral Clustering literature as the mainstream
option to choose when computing eigenvectors or eigenpairs out of the
Laplacian matrix (see \cite{luxburg07} for example). 

\subsection{LOBPCG}

The second algorithm we considered is the so called Locally Optimal
Block Preconditioned Conjugate Gradient (LOBPCG), and is also an
iterative method that applies Rayleigh-Ritz Method every time (for
simplicity we focus on the single vector version). The main difference
with Lanczos is the subspace where it searches for the eigenvector
approximations, which for LOBPCG is $\func{span} \{\vec{x}_i,
T\vec{r}_i, \vec{x}_{i-1}\}$. This is a fixed size set of 3 vectors,
containing respectively the current and previous approximations
($x_i$, $x_{i-1}$ \footnote{In practice $x_{i} - \beta x_{i-1}$ is used
  instead of the previous approximation $x_{i-1}$; as it tends to
  loose orthogonality respect to $x_i$.}) plus the preconditioned
residual $T\vec{r_i}$. The residual vector $r_i$ measures how well
$x_i$ approximates the desired eigenvector and is defined below:

\begin{equation*}
    \joinabove{0}
    \vec{r}_i = A\vec{x}_i - \func{\rho}(\vec{x}_i)\vec{x}_i
    \ds{\land}
    \func{\rho}(\vec{x}) = \dfrac{\trans{\vec{x}}A\vec{x}}{\trans{\vec{x}}\vec{x}}
\end{equation*}
\joinbelow{1cm}

The function $\func{\rho}(\vec{x})$ is nothing more than the
Rayleigh-Quotient again; and another of its properties justifies the
inclusion of the residual $r_i$. It turns out that
$\func{\rho}(\vec{x})$ has as critical point precisely the smallest
eigenpair $(\vec{v_1},\lambda_1)$; furthermore, its gradient is
proportional to $r_i$. This means that $r_i$ is proportional to the
direction where the Rayleigh-Quotient Function approximantes better
the smallest eigenpair; hence, it kind of makes sense to include such
direction when cooking the linear combinations that will approximate
the desired eigenpair. The details of this algorithms are quite
technical, but the brave reader can consult the original article from
its creator Knyazev (\cite{knyazev01}). A more digested version
appears on the PhD thesis of one of Knyazev's students (see
\cite{lashuk07}).  \\ 

The reader may have noticed a little inconsistency; above we talked about
the smallest eigenpair but was not the Fiedler Vector part of the
second one instead? That is still the case, but LOBPCG has a nice
feature that allow us to focus on the smallest eigenpair indeed. When
invoking the routine the user can pass a matrix $Y$ (called
constraints matrix) whose columns
generate certain subspace; then the algorithm seaches for the
approximations only in the orthogonal subspace $\ortc{Y}$. This fits
perfectly our problem, as we know that the first eigenvector of the
Laplacian is the vector $\vec{1}$; since we are dealing with a
symmetric matrix we know that the rest of the eigenvectors (in
particular the Fiedler Vector) are going to live inside
$\ortc{\vec{1}}$. Thus, we set $Y = \vec{1}$ when calling the LOBPCG
routine; as that makes the search of the smallest eigenpair in
$\ortc{\vec{1}}$ equivalent to the search of the second smallest on
the original space. \\

Let us summarize now all the practical considerations taken when
calling the LOBPCG implementation (which were actually taken from the
Python Package NetworkX):

\begin{itemize}
\item As mentioned, given we set the constraints matrix $Y = \vec{1}$,
  we only need to request the smallest eigenpair (set $k=1$). 
\item The matrix $T$ multiplying the residual vector $\vec{r_i}$ is called
  the preconditioner; and its purpose is to accelerate
  convergence \footnote{Going into the theory behind is out of
    our scope; but again,  the interested reader can consult \cite{knyazev01}
    or the more digested \cite{lashuk07}}. In practice, and for the
  data involved with our application, setting $T =
  \dfrac{1}{\func{diag}(A)}$ worked pretty well (actually, this is
  perhaps the most important input parameter for our case; without
  this preconditioner, LOBPCG was even slower than Lanczos). 
\item Rather than slow convergence, the implementation showed
  numerical errors on clustered eigenvalues (for some reason, the
  dense $3 \times 3$ matrix produced at each iteration eventually
  looses the properties the code expects). We did not dig further into
  this problem, but just assumed that we needed to do something to
  avoid clustered eigenvalues on the data; given that both sparse
  algorithms have problems with them.
\end{itemize}

