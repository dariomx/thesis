all for laplacian matrix which has following attributes:

0. symmetric
1. sparse
2. positive semi-definite

--------------------------------------------------------------------------------------------------------
current approach:

colt library 1.2.0 (CERN), just the serial version; this does not seem to allow for integration
with standard blas and lapack libraries (which some vendors as Intel do optimize)

1. Symmetric Householder reduction to tridiagonal form
   //  This is derived from the Algol procedures tred2 by
   //  Bowdler, Martin, Reinsch, and Wilkinson, Handbook for
   //  Auto. Comp., Vol.ii-Linear Algebra, and the corresponding
   //  Fortran subroutine in EISPACK.

2. Symmetric tridiagonal QL algorithm.
   //  This is derived from the Algol procedures tql2, by
   //  Bowdler, Martin, Reinsch, and Wilkinson, Handbook for
   //  Auto. Comp., Vol.ii-Linear Algebra, and the corresponding
   //  Fortran subroutine in EISPACK.

https://en.wikipedia.org/wiki/EISPACK

Originally written around 1972–1973,[1] EISPACK, like LINPACK and MINPACK, originated from Argonne National Laboratory, has always been free, and aims to be portable, robust and reliable. The library drew heavily on algorithms developed by James Wilkinson, which were originally implemented in ALGOL. Brian Smith led a team at Argonne developing EISPACK, initially by translating these algorithms into FORTRAN. Jack Dongarra joined the team as an undergraduate intern at Argonne, and later went on create LAPACK, which has largely superseded EISPACK and LINPACK.


above imolied routines (step 2) seems superseded:

J. J. DONGARRA, S. HAMMARLING, AND D. C. SORENSEN, Block reduction of matrices to condensed forms for eigenvalue computations, JCAM, 27 (1989), pp. 215-227. 
(LAPACK Working Note #2).

--------------------------------------------------------------------------------------------------------

LAPACK, which despite not being designed for sparse matrices, is the modern replacement of EISPACK; the
library on which Colt based its eigensolver.

Contains both driver and computational routines:


driver ones:
arbenz-notes-solving-large-scale-eigen-prob.pdf
http://www.netlib.org/lapack/lug/node30.html

This computation proceeds in the following stages:

1.
The real symmetric or complex Hermitian matrix A is reduced to real tridiagonal form T. If A is real symmetric this decomposition is A=QTQT with Q orthogonal and T symmetric tridiagonal. If A is complex Hermitian, the decomposition is A=QTQH with Q unitary and T, as before, real symmetric tridiagonal.

2.
Eigenvalues and eigenvectors of the real symmetric tridiagonal matrix T are computed. If all eigenvalues and eigenvectors are computed, this is equivalent to factorizing T as  $T = S \Lambda S^T$, where S is orthogonal and $\Lambda$ is diagonal. The diagonal entries of $\Lambda$ are the eigenvalues of T, which are also the eigenvalues of A, and the columns of S are the eigenvectors of T; the eigenvectors of A are the columns of Z=QS, so that  $A=Z \Lambda Z^T$ ($Z \Lambda Z^H$ when A is complex Hermitian).


A simple driver (name ending -EV) computes all the eigenvalues and (optionally) eigenvectors.

An expert driver (name ending -EVX) computes all or a selected subset of the eigenvalues and (optionally) eigenvectors. If few enough eigenvalues or eigenvectors are desired, the expert driver is faster than the simple driver.

A divide-and-conquer driver (name ending -EVD) solves the same problem as the simple driver. It is much faster than the simple driver for large matrices, but uses more workspace. The name divide-and-conquer refers to the underlying algorithm (see sections 2.4.4 and 3.4.3).

A relatively robust representation (RRR) driver (name ending -EVR) computes all or (in a later release) a subset of the eigenvalues, and (optionally) eigenvectors. It is the fastest algorithm of all (except for a few cases), and uses the least workspace. The name RRR refers to the underlying algorithm (see sections 2.4.4 and 3.4.3).

http://www.netlib.org/lapack/lug/node48.html#subseccompsep

In the real case, the decomposition A = Q T QT is computed by one of the routines xSYTRD, xSPTRD, or xSBTRD, depending on how the matrix is stored (xSYTRD=dense, xSPTRD=packed and xSBTRD=band)

if all eigen vectors are required, auxiliary routines for forming matrix may be required.

once in tridiagonal form, eigen values and optionally eigen vectors are calculated:

xSTEQR
This routine uses the implicitly shifted QR algorithm. It switches between the QR and QL variants in order to handle graded matrices more effectively than the simple QL variant that is provided by the EISPACK routines IMTQL1 and IMTQL2. See [56] for details. This routine is used by drivers with names ending in -EV and -EVX to compute all the eigenvalues and eigenvectors (see section 2.3.4.1).

xSTERF
This routine uses a square-root free version of the QR algorithm, also switching between QR and QL variants, and can only compute all the eigenvalues. See [56] for details. This routine is used by drivers with names ending in -EV and -EVX to compute all the eigenvalues and no eigenvectors (see section 2.3.4.1).
xSTEDC

This routine uses Cuppen's divide and conquer algorithm to find the eigenvalues and the eigenvectors (if only eigenvalues are desired, xSTEDC calls xSTERF). xSTEDC can be many times faster than xSTEQR for large matrices but needs more work space (2n2 or 3n2). See [20,57,89] and section 3.4.3 for details. This routine is used by drivers with names ending in -EVD to compute all the eigenvalues and eigenvectors (see section 2.3.4.1).

xSTEGR
This routine uses the relatively robust representation (RRR) algorithm to find eigenvalues and eigenvectors. This routine uses an LDLT factorization of a number of translates T - sI of T, for one shift s near each cluster of eigenvalues. For each translate the algorithm computes very accurate eigenpairs for the tiny eigenvalues. xSTEGR is faster than all the other routines except in a few cases, and uses the least workspace. See [35] and section 3.4.3 for details.

xPTEQR
This routine applies to symmetric positive definite tridiagonal matrices only. It uses a combination of Cholesky factorization and bidiagonal QR iteration (see xBDSQR) and may be significantly more accurate than the other routines except xSTEGR. See [14,32,23,51] for details.

xSTEBZ
This routine uses bisection to compute some or all of the eigenvalues. Options provide for computing all the eigenvalues in a real interval or all the eigenvalues from the ith to the jth largest. It can be highly accurate, but may be adjusted to run faster if lower accuracy is acceptable. This routine is used by drivers with names ending in -EVX.

xSTEIN
Given accurate eigenvalues, this routine uses inverse iteration to compute some or all of the eigenvectors. This routine is used by drivers with names ending in -EVX.


still lapack but as an example of block algorithms:
http://www.netlib.org/lapack/lug/node70.html#subsecblockeig

The first step in solving many types of eigenvalue problems is to reduce the original matrix to a condensed form by orthogonal transformations. In the reduction to condensed forms, the unblocked algorithms all use elementary Householder matrices and have good vector performance. Block forms of these algorithms have been developed [46], but all require additional operations, and a significant proportion of the work must still be performed by Level 2 BLAS, so there is less possibility of compensating for the extra operations.


reduction of a symmetric matrix to tridiagonal form to solve a symmetric eigenvalue problem: LAPACK routine xSYTRD applies a symmetric block update of the form 
\begin{displaymath}A \leftarrow A - U X^T - X U^T\end{displaymath}

using the Level 3 BLAS routine xSYR2K; Level 3 BLAS account for at most half the work.

Note that only in the reduction to Hessenberg form is it possible to use the block Householder representation described in subsection 3.4.2. Extra work must be performed to compute the n-by-b matrices X and Y that are required for the block updates (b is the block size) -- and extra workspace is needed to store them


--------------------------------------------------------------------------------------------------------
how to use lapack/blas or equiv in java?

native libs:
https://github.com/fommil/netlib-java
(above uses a java port per http://www.netlib.org/java/f2j/)

howto:
http://jeshua.me/blog/NetlibJavaJNI

java flavours:
http://dsd.lbl.gov/%7Ehoschek/colt/ (current soln)
http://commons.apache.org/proper/commons-math/ (current recomm, based on Dillon and MR3 alg)
http://math.nist.gov/javanumerics/jama/ (uses same as colt)
https://github.com/fommil/matrix-toolkits-java
http://code.google.com/p/efficient-java-matrix-library/
https://code.google.com/archive/p/sparse-eigensolvers-java/

generic info:
http://math.nist.gov/javanumerics/jama/

arpack in java!
arpack => http://www.netlib.org/java/f2j/ => https://github.com/fommil/netlib-java =>
https://github.com/fommil/matrix-toolkits-java => 
https://github.com/fommil/matrix-toolkits-java/blob/master/src/main/java/no/uib/cipr/matrix/sparse/ArpackSym.java 

arpack java uses arpack fortran which uses blas/lapack

--------------------------------------------------------------------------------------------------------
arbenz-notes-solving-large-scale-eigen-prob.pdf

2 Basics
2.1 Notation . . . . . . . . . . . . . . . . . .
2.2 Statement of the problem . . . . . . . .
2.3 Similarity transformations . . . . . . . .
2.4 Schur decomposition . . . . . . . . . . .
2.5 The real Schur decomposition . . . . . .
2.6 Normal matrices . . . . . . . . . . . . .
2.7 Hermitian matrices . . . . . . . . . . . .
2.8 Cholesky factorization . . . . . . . . . .
2.9 The singular value decomposition (SVD)
2.10 Projections . . . . . . . . . . . . . . . .
2.11 Angles between vectors and subspaces .

3 The QR Algorithm
3.1 The basic QR algorithm . . . . . . . . . . . . .
3.1.1 Numerical experiments . . . . . . . . . .
3.2 The Hessenberg QR algorithm . . . . . . . . .
3.2.1 A numerical experiment . . . . . . . . .
3.2.2 Complexity . . . . . . . . . . . . . . . .
3.3 The Householder reduction to Hessenberg form
3.3.1 Householder reflectors . . . . . . . . . .
3.3.2 Reduction to Hessenberg form . . . . .
3.4 Improving the convergence of the QR algorithm
3.4.1 A numerical example . . . . . . . . . . .
3.4.2 QR algorithm with shifts . . . . . . . .
3.4.3 A numerical example . . . . . . . . . . .
3.5 The double shift QR algorithm . . . . . . . . .
3.5.1 A numerical example . . . . . . . . . . .
3.5.2 The complexity . . . . . . . . . . . . . .
3.6 The symmetric tridiagonal QR algorithm . . .
3.6.1 Reduction to tridiagonal form . . . . . .
3.6.2 The tridiagonal QR algorithm . . . . . .
3.7 Research . . . . . . . . . . . . . . . . . . . . . .
3.8 Summary . . . . . . . . . . . . . . . . . . . . .

4 Cuppen’s Divide and Conquer Algorithm
4.1 The divide and conquer idea . . . . . . . .
4.2 Partitioning the tridiagonal matrix . . . .
4.3 Solving the small systems . . . . . . . . .
4.4 Deflation . . . . . . . . . . . . . . . . . .
4.4.1 Numerical examples . . . . . . . .
4.5 The eigenvalue problem for D + ρvv T . .
4.6 Solving the secular equation . . . . . . . .
4.7 A first algorithm . . . . . . . . . . . . . .
4.7.1 A numerical example . . . . . . . .
4.8 The algorithm of Gu and Eisenstat . . . .
4.8.1 A numerical example [continued] .

5 LAPACK and the BLAS
5.1 LAPACK . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 BLAS . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2.1 Typical performance numbers for the BLAS . . .
5.3 Blocking . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.4 LAPACK solvers for the symmetric eigenproblems . . .
5.5 Generalized Symmetric Definite Eigenproblems (GSEP)
5.6 An example of a LAPACK routines . . . . . . . . . . . .

6 Vector iteration (power method)
6.1 Simple vector iteration . . . . . . .
6.2 Convergence analysis . . . . . . . .
6.3 A numerical example . . . . . . . .
6.4 The symmetric case . . . . . . . .
6.5 Inverse vector iteration . . . . . . .
6.6 The generalized eigenvalue problem
6.7 Computing higher eigenvalues . . .
6.8 Rayleigh quotient iteration . . . .
6.8.1 A numerical example . . . .

7 Simultaneous vector or subspace iterations
7.1 Basic subspace iteration . . . . . . . . . . . . . . . . .
7.2 Convergence of basic subspace iteration . . . . . . . .
7.3 Accelerating subspace iteration . . . . . . . . . . . . .
7.4 Relation between subspace iteration and QR algorithm
7.5 Addendum . . . . . . . . . . . . . . . . . . . . . . . .

8 Krylov subspaces
8.1 Introduction . . . . . . . . . . . . . . . . . . . .
8.2 Definition and basic properties . . . . . . . . .
8.3 Polynomial representation of Krylov subspaces
8.4 Error bounds of Saad . . . . . . . . . . . . . . .

9 Arnoldi and Lanczos algorithms
9.1 An orthonormal basis for the Krylov space K j (x) . . .
9.2 Arnoldi algorithm with explicit restarts . . . . . . . .
9.3 The Lanczos basis . . . . . . . . . . . . . . . . . . . .
9.4 The Lanczos process as an iterative method . . . . . .
9.5 An error analysis of the unmodified Lanczos algorithm
9.6 Partial reorthogonalization . . . . . . . . . . . . . . .
9.7 Block Lanczos . . . . . . . . . . . . . . . . . . . . . . .
9.8 External selective reorthogonalization . . . . . . . . .

10 Restarting Arnoldi and Lanczos algorithms
10.1 The m-step Arnoldi iteration . . . . . . . .
10.2 Implicit restart . . . . . . . . . . . . . . . .
10.3 Convergence criterion . . . . . . . . . . . .
10.4 The generalized eigenvalue problem . . . . .
10.5 A numerical example . . . . . . . . . . . . .
10.6 Another numerical example . . . . . . . . .
10.7 The Lanczos algorithm with thick restarts .
10.8 Krylov–Schur algorithm . . . . . . . . . . .
10.9 The rational Krylov space method . . . . .

11 The Jacobi-Davidson Method
11.1 The Davidson algorithm . . . . . . . . . . . . . . . . . .
11.2 The Jacobi orthogonal component correction . . . . . .
11.2.1 Restarts . . . . . . . . . . . . . . . . . . . . . . .
11.2.2 The computation of several eigenvalues . . . . .
11.2.3 Spectral shifts . . . . . . . . . . . . . . . . . . .
11.3 The generalized Hermitian eigenvalue problem . . . . .
11.4 A numerical example . . . . . . . . . . . . . . . . . . . .
11.5 The Jacobi–Davidson algorithm for interior eigenvalues .
11.6 Harmonic Ritz values and vectors . . . . . . . . . . . . .
11.7 Refined Ritz vectors . . . . . . . . . . . . . . . . . . . .
11.8 The generalized Schur decomposition . . . . . . . . . . .
11.9 JDQZ: Computing a partial QZ decomposition . . . . .
11.9.1 Restart . . . . . . . . . . . . . . . . . . . . . . .
11.9.2 Deflation . . . . . . . . . . . . . . . . . . . . . .
11.9.3 Algorithm . . . . . . . . . . . . . . . . . . . . . .
11.10Jacobi-Davidson for nonlinear eigenvalue problems . . .

12 Rayleigh quotient and trace minimization
12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
12.2 The method of steepest descent . . . . . . . . . . . . . . . . . . . . . . . . . 224
12.3 The conjugate gradient algorithm . . . . . . . . . . . . . . . . . . . . . . . . 225
12.4 Locally optimal PCG (LOPCG) . . . . . . . . . . . . . . . . . . . . . . . . . 229
12.5 The block Rayleigh quotient minimization algorithm (BRQMIN) . . . . . . 232
12.6 The locally-optimal block preconditioned conjugate gradient method (LOBPCG)232
12.7 A numerical example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
12.8 Trace minimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235


***********
hammarling-software-eigenvalue-problem.pdf

- jacobi as the first (pre-computer 1946, symmetric, used in recent lapack for svd, can b more accurate than QR)
- work-hose since 1961 has been QR

Methods used by Wilkinson on Pilot Ace at NPL
• Power method
• Inverse iteration
• Muller’s method

brief history:
HSL from Harwell, 1963
BCSLIB from Boeing, 1965?
Handbook, 1971
NAG and IMSL, 1971
Eispack, 1974
MATLAB, 1978? (MATLAB 1.0, 1984)
LAPACK, 1992
ARPACK, 1998

software for DENSE problems:
LAPACK

large scale probs:
- power method
- inverse iteration
- subspace methods
  • Subspace Iteration (fixed size subspace)
    - Sometimes called simultaneous iteration
  • Krylov and Davidson subspace methods (increasing subspace)
    - Lanczos and block Lanczos
    - Davidson, block Davidson, Jacobi-Davidson

Balancing. Disussed in the Templates book (two slides
hence)
Preconditioning. Discussed in the Templates book
Much detail


soft for large scale probs:

subspace iteration:
 Symmetric matrix problems
  - EA22 in HSL 2011
  - SIMITZ by Nikolai, TOMS Algorithm 538, originallybased on ritzit by Rutishauser (Wilkinson and
    Reinsch, 1971)
  - F02FJF in the NAG Library, based on SIMITZ
  - Templates book

arnoldi and laczos:
  • Hermitian and symmetric matrix problems
    - ARPACK. Uses the Implicitly Restarted LanczosMethod
    - EA16 and EA25 in HSL 2011, symmetric
    - Templates book

jacobi-davison:
- Templates book
• Jacobi-Davidson web site at CASA (www.casa.tue.nl)

***********
manguoglu-tracemin-fiedler.pdf

***********
hernandez-a-survey-of-software-sparse-eigenprob.pdf

2.1 Single and Multiple Vector Iteration Methods
Single-vector iteration methods are available in the following packages:
• napack provides a version of the power iteration that can handle complex conjugate
eigenvalues. (napack also includes a simple implementation of Lanczos, see 2.3.)

• ietl implements the power iteration, inverse iteration and Rayleigh quotient iteration.
(ietl also includes a Lanczos solver, see 2.3.)

A version of subspace iteration for symmetric matrices is Rutishauser’s Ritzit procedure,
which was translated to Fortran in svdpack (see 2.4).

2.2 Arnoldi Methods
-arncheb [Braconnier, 1993] is a Fortran software that implements the Arnoldi method with
explicit restart, combined with Chebyshev polynomial acceleration.

-arpack [Lehoucq et al., 1998] provides a Fortran implementation of the Implicitly Restarted
Arnoldi method, for both real and complex arithmetic. It can be used for both standard and
generalized problems and for both symmetric and non-symmetric problems. In the symmetric
case, Lanczos with full reorthogonalization is used instead of Arnoldi. arpack is one of the
most popular eigensolvers, due to its efficiency and robustness. arpack++ is a C++ interface
to arpack.

- Another implementation of Arnoldi is also available in irbleigs (described in 2.3), which
can solve standard or generalized eigenproblems.

- A generalization of block Arnoldi with implicit restart is the block Krylov-Schur algorithm,
which is available in the anasazi eigensolver package. anasazi is part of trilinos, a parallel
object-oriented software framework for large-scale multi-physics scientific applications. Other
eigensolvers available in anasazi are block Davidson (see 2.5) and LOBPCG (see 2.6).

2.3 Lanczos Methods

Implementation of Lanczos methods abound, most of them oriented to real generalized sym-
metric eigenvalue problems. Packages specific for the computation of the singular value decom-
position (SVD) are described in section 2.4.

Basic reorthogonalization strategies 

-The Lanczos method with full reorthogonalization
can be found in the arpack package, which incorporates an implicit restart technique (see 2.2).

- An implementation of the block Lanczos method that also employs full reorthogonalization is
available in the underwood subroutines [Golub and Underwood, 1977].

- The lanczos software implements the strategy proposed by Cullum and Willoughby [1985]
in which no reorthogonalization is carried out and the resulting tridiagonal matrix is post-
processed in order to eliminate spurious eigenvalues. This is also the strategy of the Lanczos
solver found in ietl (described in 2.1). 

- On the other hand, napack (see 2.1) provides a straightforward Lanczos implementation with no reorthogonalization and no post-processing.

- The laso package implements a block Lanczos method with selective reorthogonalization
[Parlett and Scott, 1979].

Partial reorthogonalization 

The partial reorthogonalization idea is present in a number of
packages: lanz, blzpack, propack, and blklan.

- lanz [Jones and Patrick, 1993] is a shared-memory parallel Lanczos with partial reorthog-
onalization that is also intended for real generalized problems. It incorporates subroutines for
the spectral transformation and computation of inertia, and allows to specify a computational
interval in which the solutions are to be sought.

- blzpack [Marques, 1995] is an MPI-based parallel implementation of Lanczos, also for real
generalized eigenproblems. The algorithm that it provides is a block method combining both
partial and selective reorthogonalization. Computational intervals are also allowed.

- propack and blklan are packages for the computation of the SVD that also employ partial
reorthogonalization, see 2.4.

Restarting schemes 

- Some packages provide a restarting mechanism in order to limit the
required amount of memory. arpack incorporates implicit restart, as mentioned above.

- irbleigs [Baglama et al., 2003] is a Matlab program that implements an implicitly restarted
block Lanczos method, that allows the computation of extreme eigenvalues of symmetric ma-
trices or symmetric positive-definite pencils. Also, interior eigenvalues can be found without
requiring a factorization.

- trlan is based on a different restarting scheme called thick restart [Wu and Simon, 2000].
trlan is a parallel software written in Fortran 90 that can be used to address standard real
symmetric problems.

Non-SPD problems Except for the SVD packages described in next section, all the above
Lanczos solvers are intended for symmetric positive-definite problems, i.e., real symmetric or
complex Hermitian matrices and symmetric positive-definite matrix pairs. The following pack-
ages address other kind of problems.

2.4 SVD 

2.5 Davidson and Jacobi-Davidson Methods

- The package dvdson [Stathopoulos and Fischer, 1994] is a block implementation of the David-
son method with several extensions such as reorthogonalization. It is intended for real symmetric
matrices.

- The package na18 [Sadkane and Sidje, 1999] is a Fortran-77 software package which imple-
ments a deflated and variable-block version of the Davidson method for computing a few of the
extreme (i.e., leftmost or rightmost) eigenpairs of large sparse symmetric matrices.

- A parallel block Davidson for symmetric problems is included in anasazi (described in 2.2).

- Another implementation of Davidson’s method can be found in mpb (described in 2.6).

- jdqr is a Matlab implementation of the Jacobi-Davidson method for the computation of
eigenpairs of non-symmetric matrices, as described in [Fokkema et al., 1998]. The corresponding
algorithm for matrix pairs is implemented in another package called jdqz, which is also available
in Fortran with complex arithmetic.

- jdcg is a modified version of jdqr for symmetric problems, based on [Notay, 2002].

- pysparse is a Python toolkit that provides a module jdsym that implements the Jacobi-
Davidson method for symmetric generalized eigenproblems.

https://mail.scipy.org/pipermail/scipy-user/2011-September/030660.html
https://www.quora.com/Which-Python-library-is-better-for-simple-sparse-matrix-computations-PySparse-or-SciPy
http://scipy-dev.scipy.narkive.com/XkzOnzca/scipy-sparse-vs-pysparse

- primme [Stathopoulos, 2007] is a C library for finding a number of eigenvalues and their
corresponding eigenvectors of a real symmetric (or complex Hermitian) matrix. This library
provides a multimethod eigensolver, based on Davidson/Jacobi-Davidson. Particular methods
include GD+1, JDQMR, and LOBPCG. It supports preconditioning as well as the computation
of interior eigenvalues.

- jadamilu [Bollhüofer and Notay, 2007] is an implementation of the Jacobi-Davidson method
for computing smallest or interior eigenvalues of symmetric matrices. It provides a built-in
preconditioner based on ILUPACK.

2.6 Optimization and Preconditioned Methods

- mpb [Johnson and Joannopoulos, 2001] is a package specific for electromagnetics simulation
that includes two eigensolvers: preconditioned conjugate-gradient Rayleigh-quotient minimiza-
tion and Davidson’s method.

- pdacg is a parallel implementation of the Deflation-Accelerated Conjugate Gradient (DACG)
method [Gambolati et al., 1992], to compute the smallest eigenpairs of a symmetric positive-
definite matrix pair.

- blopex provides the Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG)
method [Knyazev, 2001]. This package comes in several flavors: a Matlab function, a serial C
library and a parallel C library to be combined with either Hypre or PETSc. 

- The LOBPCG method is also available in anasazi (described in 2.2).
https://github.com/scipy/scipy/issues/3592

- eigifp is a Matlab program that computes the smallest or largest eigenpairs of a symmetric
matrix or a symmetric positive-definite matrix pair. It is based on the inverse free precondi-
tioned Krylov subspace method [Golub and Ye, 2003].

- spam implements the Subspace Projected Approximating Matrices technique for extend-
ing Davidson’s method, as described in [Shepard et al., 2001]. This method is applicable to
symmetric eigenproblems.

--------------------------------------------------------------------------------------------------------

***********
anazi:
Unlike ARPACK, which provides a single eigensolver, Anasazi provides a framework capable of describing a wide variety of eigenproblems and algorithms for solving them. Anasazi can currently solve complex and real, Hermitian and non-Hermitian, eigenvalue problems, via the following included methods:

    Block Krylov-Schur method, a block extension of A Krylov-Schur Algorithm for Large Eigenproblems, G. W. Stewart, SIAM J. Matrix Anal. Appl., 23, pp. 601-614 (2000).

    Block Davidson method described in A Comparison of Eigensolvers for Large-scale 3D Modal Analysis Using AMG-Preconditioned Iterative Methods, P. Arbenz, U. L. Hetmaniuk, R. B. Lehoucq, R. S. Tuminaro, Internat. J. for Numer. Methods Engrg., 64, pp. 204-236 (2005).

    LOBPCG, a stable implementation of Toward the optimal preconditioned eigensolver: Locally optimal block preconditioned conjugate gradient method, SIAM J. Sci. Comput., 23 (2001), pp. 517-541, as described in Basis selection in LOBPCG, U. L. Hetmaniuk and R. B. Lehoucq, J. Comput. Physics, 218, pp. 324-332 (2006)

    IRTR, an implicit version of the Riemannian Trust-Region Eigensolver, orginally described in A truncated-CG style method for symmetric generalized eigenvalue problems, P.-A. Absil, C. G. Baker, and K. A. Gallivan, J. Computational and Applied Mathematics, 189, pp. 274-285 (2006).

    Generalized Davidson method, a non-symmetric variant of the block Davidson solver described above. This solver is experimental, but is currently in active use. It currently only supports real-valued scalar types.

    TraceMin method described in A trace minimization algorithm for the generalized eigenvalue problem, A. Sameh and J. Wisniewski, SIAM J. Numer. Anal., 19, pp. 1243-1259 (1982). This solver is experimental.

    TraceMin-Davidson method described in The trace minimization method for the symmetric generalized eigenvalue problem, A. Sameh and Z. Tong, J. Computational and Applied Mathematics, 123, pp. 155-175 (2000). This solver is experimental.

**************
slepc:
Basic methods
  – Power Iteration with deflation. When combined with shift-and-invert (see chapter
    3), it is equivalent to the Inverse Iteration. Also, this solver embeds the Rayleigh
    Quotient Iteration (RQI) by allowing variable shifts.
  – Subspace Iteration with Rayleigh-Ritz projection and locking.
  – Arnoldi method with explicit restart and deflation.
  – Lanczos with explicit restart, deflation, and different reorthogonalization strategies.
• Krylov-Schur, a variation of Arnoldi with a very effective restarting technique. In the case
of symmetric problems, this is equivalent to the thick-restart Lanczos method.
• Generalized Davidson, a simple iteration based on the subspace expansion by the precon-
ditioned residual.
• Jacobi-Davidson, a preconditioned eigensolver with an effective correction equation.
• RQCG, a basic conjugate gradient iteration for the minimization of the Rayleigh quotient.
• LOBPCG, the locally-optimal block preconditioned conjugate gradient.
• CISS, a contour-integral solver that allows computing all eigenvalues in a given region.

**************
JADAMILU
Note that, instead of Jacobi–Davidson, we could have selected the pre-
conditioned Lanczos method [29], a Generalized Davidson
method [30,31], and in particular the “GD+1” variant from
[32,33], or a method based on nonlinear conjugate gradi-
ent [34], in particular LOBPCG [35] which satisfies an op-
timality property. All of these methods share with JD the
characteristic that they are not easy to implement (espe-
cially taking into account stability issues [36]) and require
preconditioning for efficiency. On average, the best alterna-
tives perform similarly to JD when computing the smallest
eigenvalue [37,32]. We selected JD because the inner-outer
scheme makes it potentially more robust, in particular for
computing interior eigenvalues, a case for which other ap-
proaches are not so well suited.

**************
PRIMME
GD+k and JDQMR
General-Davison+k and Jacob-Davison (extend JDCG) + QMR
--------------------------------------------------------------------------------------------------------
stathopoulos-primme-preecond-iter-multimet.pdf (2010)
PRIMME: PReconditioned Iterative MultiMethod
Eigensolver: Methods and software description

The Anasazi and SLEPc packages come close to a robust, efficient, general pur-
pose code, but they do not offer all the state-of-the-art methods and could be bet-
ter viewed as platforms for further development. In JADAMILU the near-optimal
method comes constrained by a provided preconditioner and a distribution which
is far less general purpose. Our goal in this software project has been to bring
state-of-the-art methods and expertise from “bleeding edge” to production.

Anazi (C++):
https://trilinos.org/docs/r12.4/packages/anasazi/doc/html/index.html

Slepc (C):
slepc-user-manual.pdf
http://slepc.upv.es/documentation/manual.htm

JADAMILU (fortran77):
JADAMILU: a software code for computing selected eigenvalues of large
sparse symmetric matrices
http://homepages.ulb.ac.be/~jadamilu/

------------------
NAG
http://www.nag.co.uk/numeric/full_genconts.html#F02

For dense or band matrices, the computation of eigenvalues and eigenvectors proceeds in the following stages:

1. 	A is reduced to a symmetric tridiagonal matrix T by an orthogonal similarity transformation: A=QTQT, where Q is orthogonal. (A tridiagonal matrix is zero except for the main diagonal and the first subdiagonal and superdiagonal on either side.) T has the same eigenvalues as A and is easier to handle.

2. 	Eigenvalues and eigenvectors of T are computed as required. If all eigenvalues (and optionally eigenvectors) are required, they are computed by the QR algorithm, which effectively factorizes T as T=SΛST, where S is orthogonal, or by the divide-and-conquer method. If only selected eigenvalues are required, they are computed by bisection, and if selected eigenvectors are required, they are computed by inverse iteration. If s is an eigenvector of T, then Qs is an eigenvector of A.

All the above remarks also apply – with the obvious changes – to the case when A is a complex Hermitian matrix. The eigenvectors are complex, but the eigenvalues are all real, and so is the tridiagonal matrix T.
If A is large and sparse, the methods just described would be very wasteful in both storage and computing time, and therefore an alternative algorithm, known as subspace iteration, is provided (for real problems only) to find a (usually small) subset of the eigenvalues and their corresponding eigenvectors. Chapter f12 contains functions based on the Lanczos method for real symmetric large sparse eigenvalue problems, and these functions are usually more efficient than subspace iteration.



https://software.intel.com/en-us/articles/introduction-to-the-intel-mkl-extended-eigensolver


http://www.luigidragone.com/software/spectral-clusterer-for-weka/ (does not use lanczos)


https://github.com/scikit-learn/scikit-learn/blob/51a765a/sklearn/cluster/spectral.py#L269
 eigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}
        The eigenvalue decomposition strategy to use. AMG requires pyamg
        to be installed. It can be faster on very large, sparse problems,
        but may also lead to instabilities

http://www.cs.utexas.edu/users/dml/Software/graclus.html (Dhillon!)
Graclus (latest: Version 1.2) is a fast graph clustering software that computes normalized cut and ratio association for a given undirected graph without any eigenvector computation. This is possible because of the mathematical equivalence between general cut or association objectives (including normalized cut and ratio association) and the weighted kernel k-means objective. One important implication of this equivalence is that we can run a k-means type of iterative algorithm to minimize general cut or association objectives. Therefore unlike spectral methods, our algorithm totally avoids time-consuming eigenvector computation. We have embedded the weighted kernel k-means algorithm in a multilevel framework to develop very fast software for graph clustering.


http://java-ml.sourceforge.net/
https://sites.google.com/site/qianmingjie/home/toolkits/jml


